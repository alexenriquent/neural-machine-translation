# Neural Machine Translation with a Transformer
The Transformer was originally proposed in ["Attention is all you need"](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017). This project focuses on creating and training a Transformer model for translating one language to another. Its sequence-to-sequence encoder-decoder architecture is similar to that of the that encoder-decoder RNN model. The main difference is that the recurrent layers are replaced with self-attention layers, which allows the model to easily transmit information across the input sequences.
